{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - pandas Molly McVicar \n",
    "### due 3/1/2022   \n",
    "\n",
    "Use this notebook and prompts to complete the homework. Throughout there will be hints and some code provided  \n",
    "\n",
    "### Things you will need:  \n",
    "- Install os, NumPy, pandas  \n",
    "- states_covid.csv  \n",
    "- Bloom_etal_2018_Reduced_Dataset.csv  \n",
    "- logfiles.tgz (or some other multiple file dataset)  \n",
    "\n",
    "*NOTE*: Make sure your PATH is correct  \n",
    "\n",
    "**import packages & check required datasets**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'YOUR/PATH/HERE' #CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(PATH,'states_covid.csv')), 'states_covid.csv does not exist' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(PATH,'Bloom_etal_2018_Reduced_Dataset.csv')), 'Bloom_etal_2018_Reduced_Dataset.csv does not exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - DataFrame manipulation  \n",
    "\n",
    "Using **states_covid.csv**, we are going to read the data in as a DataFrame to manipulate, subset, and filter in various ways.   \n",
    "\n",
    "### Task 1.1 \n",
    "\n",
    "Read in states_covid.csv with date as a \"date\" dtype, and only columns consisting of the hospitalization (4 col), ICU (2 col), and Ventilators (2 col)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date  hospitalized  inIcuCumulative  onVentilatorCumulative\n",
      "0      2021-02-23        1260.0              NaN                     NaN\n",
      "1      2021-02-23       45250.0           2632.0                  1497.0\n",
      "2      2021-02-23       14617.0              NaN                  1505.0\n",
      "3      2021-02-23           NaN              NaN                     NaN\n",
      "4      2021-02-23       57072.0              NaN                     NaN\n",
      "...           ...           ...              ...                     ...\n",
      "20103  2020-01-17           NaN              NaN                     NaN\n",
      "20104  2020-01-16           NaN              NaN                     NaN\n",
      "20105  2020-01-15           NaN              NaN                     NaN\n",
      "20106  2020-01-14           NaN              NaN                     NaN\n",
      "20107  2020-01-13           NaN              NaN                     NaN\n",
      "\n",
      "[20108 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "\n",
    "covids_df = pd.read_csv('states_covid.csv') # read in file\n",
    "\n",
    "new_covids_df = covids_df[['date','hospitalized','inIcuCumulative','onVentilatorCumulative']] # read in specific columns \n",
    "print(new_covids_df) # print in terminal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 \n",
    "\n",
    "For each of the following catergories: *currently* hospitalized, *currently* in the ICU, and *currently* on ventilation...  \n",
    "Find the 5 states with the greatest numbers in each catergory and list them in order.      \n",
    "*hint*: sort_values, unique  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>hospitalizedCurrently</th>\n",
       "      <th>inIcuCurrently</th>\n",
       "      <th>onVentilatorCurrently</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CA</td>\n",
       "      <td>7749.398204</td>\n",
       "      <td>1948.476048</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TX</td>\n",
       "      <td>6251.187879</td>\n",
       "      <td>2274.571429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FL</td>\n",
       "      <td>4829.707424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NY</td>\n",
       "      <td>4506.904070</td>\n",
       "      <td>1107.038806</td>\n",
       "      <td>449.324232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>IL</td>\n",
       "      <td>2965.941176</td>\n",
       "      <td>687.046440</td>\n",
       "      <td>368.229102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state  hospitalizedCurrently  inIcuCurrently  onVentilatorCurrently\n",
       "5     CA            7749.398204     1948.476048                    NaN\n",
       "47    TX            6251.187879     2274.571429                    NaN\n",
       "10    FL            4829.707424             NaN                    NaN\n",
       "37    NY            4506.904070     1107.038806             449.324232\n",
       "16    IL            2965.941176      687.046440             368.229102"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "\n",
    "covids_df = pd.read_csv('states_covid.csv', usecols=['state', 'hospitalizedCurrently', 'inIcuCurrently', 'onVentilatorCurrently']) # read in csv file and specific columns\n",
    "\n",
    "covids_sub_df = covids_df[['state', 'hospitalizedCurrently', 'inIcuCurrently', 'onVentilatorCurrently']] # sort by specific columns \n",
    "covid_state = covids_sub_df.groupby(['state'],as_index=False).mean() #group columns by state column                          \n",
    "covid_state.nlargest(5, ['hospitalizedCurrently', 'inIcuCurrently', 'onVentilatorCurrently']) # 5 states with greatest number in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 \n",
    "\n",
    "Find the date in which each state crossed 1000 cumulative hospitilized covid patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>hospitalizedCumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>NY</td>\n",
       "      <td>1531.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>NY</td>\n",
       "      <td>2354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>NY</td>\n",
       "      <td>3125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>NY</td>\n",
       "      <td>4041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>NY</td>\n",
       "      <td>5126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12430</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>UT</td>\n",
       "      <td>14520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12431</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>VA</td>\n",
       "      <td>23698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12432</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>WA</td>\n",
       "      <td>19110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12433</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>WI</td>\n",
       "      <td>25838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12434</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>WY</td>\n",
       "      <td>1373.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date state  hospitalizedCumulative\n",
       "41     2020-03-21    NY                  1531.0\n",
       "55     2020-03-22    NY                  2354.0\n",
       "69     2020-03-23    NY                  3125.0\n",
       "87     2020-03-24    NY                  4041.0\n",
       "106    2020-03-25    NY                  5126.0\n",
       "...           ...   ...                     ...\n",
       "12430  2021-02-23    UT                 14520.0\n",
       "12431  2021-02-23    VA                 23698.0\n",
       "12432  2021-02-23    WA                 19110.0\n",
       "12433  2021-02-23    WI                 25838.0\n",
       "12434  2021-02-23    WY                  1373.0\n",
       "\n",
       "[9303 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "\n",
    "hospitals = pd.read_csv('states_covid.csv', usecols=['date','state', 'hospitalizedCumulative']) # read in csv and columns to be used\n",
    "hospitals_sub = hospitals.groupby(['date', 'state', 'hospitalizedCumulative'],as_index=False).mean() # group columns date, state, and cumulative hospitalizations\n",
    "hospitals_sub[hospitals_sub.hospitalizedCumulative > 1000] # dates with hospitalizations above 1000\n",
    "\n",
    "## could not get one per state ##\n",
    "\n",
    "hospitals_sub = hospitals.groupby('date')['state','hospitalizedCumulative'].agg(['unique']) # tried to aggregate columns for unqiue ID with this, didn't allow me "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - DataFrame summarizing  \n",
    "\n",
    "Using **Bloom_etal_2018_Reduced_Dataset.csv**, we are going to do more dataframe manipulation and subsetting and summarizing    \n",
    "\n",
    "### Task 2.1 \n",
    "\n",
    "Read in Bloom_etal_2018_Reduced_Dataset.csv and create two new columns ('genus','species') that consists of the column *taxa* split at the underscore. Print out the head of this new dataframe and the number of unique genera**     \n",
    "\n",
    "*hint:* pd.str.split(,expand=True)  \n",
    "\n",
    "for example:  \n",
    "\n",
    "| taxa | genus | species   \n",
    "| :------ | :-- | :---   \n",
    "| Alosa_alabamae | Alosa | alabamae  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   genus         species\n",
      "0  Alosa        alabamae\n",
      "1  Alosa           alosa\n",
      "2  Alosa          fallax\n",
      "3  Alosa       mediocris\n",
      "4  Alosa  pseudoharengus\n",
      "No.of.unique genera : 34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bloom_df = pd.read_csv('Bloom_etal_2018_Reduced_Dataset.csv') # read in csv file \n",
    "newbloom = pd.DataFrame(bloom_df.taxa.str.split('_',1).tolist(),\n",
    "                         columns = ['genus','species']) # seperate genus and species \n",
    "print(newbloom.head()) # print out first 5 of genus and species seperate columns\n",
    "genera = len(pd.unique(newbloom['genus'])) # number of unique genera\n",
    "\n",
    "print(\"No.of.unique genera :\",genera) # print above command\n",
    "\n",
    "## was not able to keep taxa column, kept getting \"columns must be the same length as key\" error and could not find code to fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 \n",
    "\n",
    "Create a new dataframe with the mean *logbodysize* and *trophicposition* of each genera. Sort this data frame by the largest body size. Print the head of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 taxa  logbodysize  trophic_position\n",
      "5   Alosa_sapidissima     1.790285          0.544068\n",
      "1         Alosa_alosa     1.778151          0.556303\n",
      "2        Alosa_fallax     1.778151          0.556303\n",
      "3     Alosa_mediocris     1.778151          0.612784\n",
      "48   Tenualosa_ilisha     1.778151          0.462398\n"
     ]
    }
   ],
   "source": [
    "## due to not being able to add split columns to past data set, I had to use normal data set and do by taxa and not genus specifically ##\n",
    "\n",
    "bloom_df = pd.read_csv('Bloom_etal_2018_Reduced_Dataset.csv') # read in csv file \n",
    "largest_df = bloom_df.sort_values([\"logbodysize\"], ascending=False).iloc[:,[0,1,2]].head() # data frame sorting by descending body size, showing taxa, logbodysize, and trophic_position, and the first 5\n",
    "print(largest_df) # print above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which genera is the smallest and largest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'taxa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1d/jf6yy8lj4ks48h46sm2hqld40000gn/T/ipykernel_6039/1957171263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbloom_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'taxa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbloom_size_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewbloom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taxa'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'taxa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   7629\u001b[0m         \u001b[0;31m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7630\u001b[0m         \u001b[0;31m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7631\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   7632\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7633\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m    890\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    860\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'taxa'"
     ]
    }
   ],
   "source": [
    "## like I said above, I could not seperate out genera properly in the main dataframe, so this is what I would do below ##\n",
    "\n",
    "bloom_df = pd.read_csv('Bloom_etal_2018_Reduced_Dataset.csv') # read in csv file\n",
    "\n",
    "bloom_df['taxa'].unique() # unique taxa sorting\n",
    "bloom_size_df = newbloom.groupby('logbodysize',as_index=True).agg(['genus']) # associate genus and log body size\n",
    "print(bloom_size_df.nsmallest(1, ['logbodysize'])) # smallest genus\n",
    "print(bloom_size_df.nlargest(1, ['logbodysize'])) # largest genus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the trophic position of the smallest and largest?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      taxa  logbodysize  trophic_position             Reg\n",
      "21  Cetengraulis_edentulus     1.113943          0.322219  non-diadromous\n",
      "                 taxa  logbodysize  trophic_position             Reg\n",
      "36  Pellona_harroweri     1.113943          0.623249  non-diadromous\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(bloom_df.nsmallest(1, ['trophic_position'])) # smallest in trophic_position column\n",
    "print(bloom_df.nlargest(1, ['trophic_position'])) # largest in trophic_position column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Read in muliple files to a dictionary and make a DataFrame - **OPTIONAL/BONUS**  \n",
    "\n",
    "### This is not something you are expected to do in this course, but just here to give you an idea of the things that you COULD do. Answers will be posted after due date.  \n",
    "\n",
    "\n",
    "Using **logfiles**: we are going to do read in each file, get some data, append it to a dictionary to later make into a dataframe.     \n",
    "\n",
    "**note:** *make sure to unzip logfiles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/trevorfaske/Desktop/Classes/DataScience/pandas/Spring22\n"
     ]
    }
   ],
   "source": [
    "cd $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf logfiles.tgz #unzip logfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(PATH,'logfiles')\n",
    "assert os.path.exists(log_dir), 'log_dir does not exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to find the necessary files. The number of files in the log files is 36, make sure you have that many as well  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      36\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l logfiles/*txt | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/trevorfaske/g/Classes/DataScience/pandas/Spring22/logfiles/1901302121_H8_S_14.txt.txt\n"
     ]
    }
   ],
   "source": [
    "logfiles = !find $log_dir -name '*txt' #unix command to find files in log_dir directory\n",
    "logfiles = [os.path.abspath(x) for x in logfiles] #this finds the full path to the file\n",
    "print(logfiles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(logfiles)==36), 'Do not have correct number of logfiles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a little tricky here  \n",
    "\n",
    "Read in each of the logfiles, for each file extract:  \n",
    "- minimum temperature  \n",
    "- maximum temperature  \n",
    "- date of minimum temp   \n",
    "- date of maximum temp   \n",
    "- mean temp for each file.   \n",
    "\n",
    "This data should all be appended for a dictionary within a for loop:    \n",
    "Key should be the file name without the path or .txt extension  \n",
    "Values should be (minTemp,maxTemp,minDate,maxDate,meanTemp)\n",
    "\n",
    "I recommend making this work for one file first, then putting the rest in a for loop to do the rest.  \n",
    "\n",
    "Below is an example of how to read in one file\n",
    "\n",
    "*hint:* do not read date in as date object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>8:00:00 AM</td>\n",
       "      <td>47.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>8:35:00 AM</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>9:10:00 AM</td>\n",
       "      <td>48.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>9:45:00 AM</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>10:20:00 AM</td>\n",
       "      <td>50.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date         Time  Temp\n",
       "0  9/16/2013   8:00:00 AM  47.9\n",
       "1  9/16/2013   8:35:00 AM  48.2\n",
       "2  9/16/2013   9:10:00 AM  48.7\n",
       "3  9/16/2013   9:45:00 AM  49.4\n",
       "4  9/16/2013  10:20:00 AM  50.3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### set up data frame as you read in each file\n",
    "infile = pd.read_csv(logfiles[0],sep='\\t',engine='python')\n",
    "infile.columns = ['Index','Date','Time','Temp','Type']\n",
    "infile = infile[['Date','Time','Temp']]\n",
    "infile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do everything in steps, make sure it works. Calculate summaries with this one infile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-d888355c2242>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-d888355c2242>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    minTemp =\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "minTemp =   \n",
    "maxTemp =  \n",
    "minDate = infile['Date'][infile['Temp'] == infile['Temp'].min()].unique()[0] #use this for minDate \n",
    "maxDate = infile['Date'][infile['Temp'] == infile['Temp'].max()].unique()[0] #use this for maxDate \n",
    "meanTemp = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you started, I suggest writing some dummy code in plain words to help outline your for loop:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfiles_dict = {}  \n",
    "for f in logfiles:  \n",
    "    #do something   \n",
    "    #do not read date in as date object  \n",
    "    #do more something   \n",
    "    #do other stuff  \n",
    "    #make print statements EVERYWHERE  \n",
    "    #append to dict  \n",
    "    #blahbahblah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do the real code below here. You don't need to turnin your thoughts. Just put it in there as a help reminder. Most people all still do this, no matter how advanced they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once you have created a DataFrame with all the logfiles, print the head and save it to an outfile using pd.to_csv() as logfiles_df.csv** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is an example of the final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minTemp</th>\n",
       "      <th>maxTemp</th>\n",
       "      <th>minDate</th>\n",
       "      <th>maxDate</th>\n",
       "      <th>meanTemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1901302235_H6_S_14</th>\n",
       "      <td>21.3</td>\n",
       "      <td>65.8</td>\n",
       "      <td>12/6/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>35.726191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302121_H8_S_14</th>\n",
       "      <td>17.9</td>\n",
       "      <td>73.4</td>\n",
       "      <td>10/28/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>34.698866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302217_H18_D_14</th>\n",
       "      <td>21.0</td>\n",
       "      <td>82.4</td>\n",
       "      <td>10/28/2013</td>\n",
       "      <td>7/26/2014</td>\n",
       "      <td>33.334564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302110_H13_S_14</th>\n",
       "      <td>22.8</td>\n",
       "      <td>60.9</td>\n",
       "      <td>10/4/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>32.240032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302146_H14_D_14</th>\n",
       "      <td>24.6</td>\n",
       "      <td>66.1</td>\n",
       "      <td>1/21/2014</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>34.242075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     minTemp  maxTemp     minDate    maxDate   meanTemp\n",
       "1901302235_H6_S_14      21.3     65.8   12/6/2013  7/13/2014  35.726191\n",
       "1901302121_H8_S_14      17.9     73.4  10/28/2013  7/13/2014  34.698866\n",
       "1901302217_H18_D_14     21.0     82.4  10/28/2013  7/26/2014  33.334564\n",
       "1901302110_H13_S_14     22.8     60.9   10/4/2013  7/13/2014  32.240032\n",
       "1901302146_H14_D_14     24.6     66.1   1/21/2014  7/13/2014  34.242075"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logfiles_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
